custom:
  basic-cluster-props: &basic-cluster-props
    spark_version: "13.1.x-scala2.12"

  basic-static-cluster: &basic-static-cluster
    new_cluster:
      <<: *basic-cluster-props
      num_workers: 1
      node_type_id: "Standard_DS3_v2"

build:
  no_build: true

environments:
  default:
    workflows:
      #######################################################################################
      # this is an example job with single ETL task based on 2.1 API and wheel_task format #
      ######################################################################################

      # TODO: Figure out this
      # What is etl?
      # What is entry point?
      # DO I need to write some code??

      - name: "workflow-1"
        tasks:
          - task_key: "wf1-t1"
            <<: *basic-static-cluster
            python_wheel_task:
              package_name: "dbx_demo_job"
              entry_point: "bricks" # take a look at the setup.py entry_points section for details on how to define an entrypoint
              # parameters: ["--conf-file", "file:fuse://conf/tasks/sample_etl_config.yml"]
