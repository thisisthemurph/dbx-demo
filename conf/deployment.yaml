custom:
  basic-cluster-props: &basic-cluster-props
    spark_version: "13.1.x-scala2.12"

  basic-static-cluster: &basic-static-cluster
    new_cluster:
      <<: *basic-cluster-props
      num_workers: 1
      node_type_id: "Standard_DS3_v2"

build:
  no_build: true

environments:
  default:
    workflows:
      # - name: "workflow-1"
      #   tasks:
      #     - task_key: "wf1-t1"
      #       <<: *basic-static-cluster
      #       python_wheel_task:
      #         package_name: "dbx_demo_job"
      #         entry_point: "bricks" # take a look at the setup.py entry_points section for details on how to define an entrypoint
      #         # parameters: ["--conf-file", "file:fuse://conf/tasks/sample_etl_config.yml"]
      
      - name: "workflow-2"
        git_source:
          git_url: "https://github.com/thisisthemurph/dbx-demo.git"
          git_provider: "github"
          git_branch: "main"
        tasks:
          - task_key: "wf2-t1"
            existing_cluster_id: "0620-125415-q7klmhol"
            spark_python_task: 
              source: 
              python_file: "test_project/tasks/dbx-demo-job.py"

